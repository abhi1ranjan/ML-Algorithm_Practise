{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 19663,
          "sourceType": "datasetVersion",
          "datasetId": 14701
        }
      ],
      "dockerImageVersionId": 29271,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Gaussian Mixture Models Clustering",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhi1ranjan/ML-Algorithm_Practise/blob/main/Gaussian_Mixture_Models_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "arjunbhasin2013_ccdata_path = kagglehub.dataset_download('arjunbhasin2013/ccdata')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "TaM73Ch9aSkN",
        "outputId": "33268263-223b-4321-87ce-ab72b73511b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.4)\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/arjunbhasin2013/ccdata?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 340k/340k [00:00<00:00, 16.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Data source import complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <u> Gaussian Mixture Models Clustering Algorithm </u>\n",
        "\n",
        "Gaussian mixture models can be used to cluster unlabeled data in much the same way as k-means. There are, however, a couple of advantages to using Gaussian mixture models over k-means.\n",
        "\n",
        "- k-means does not account for variance(width of the bell shape curve). In two dimensions, variance/ covariance determines the shape of the distribution.\n",
        "\n",
        "![0_tBFK650dBxOxqn2H.png](attachment:0_tBFK650dBxOxqn2H.png)\n",
        "\n",
        "k-means model places a circle (or, in higher dimensions, a hyper-sphere) at the center of each cluster, with a radius defined by the most distant point in the cluster.\n",
        "\n",
        "![Screen%20Shot%202019-09-06%20at%2021.18.04.png](attachment:Screen%20Shot%202019-09-06%20at%2021.18.04.png)\n",
        "\n",
        "This works fine for when data is circular. However, when data takes on different shape, we end up with something like this.\n",
        "\n",
        "![Screen%20Shot%202019-09-06%20at%2021.18.48.png](attachment:Screen%20Shot%202019-09-06%20at%2021.18.48.png)\n",
        "\n",
        "In contrast, Gaussian mixture models can handle even very oblong clusters.\n",
        "\n",
        "![Screen%20Shot%202019-09-06%20at%2021.19.25.png](attachment:Screen%20Shot%202019-09-06%20at%2021.19.25.png)\n",
        "\n",
        "-  K-means performs hard classification whereas GMM performs soft classification, i.e. in k-means, data point is deterministically assigned to one and only one cluster, but in reality there may be overlapping between the cluster  GMM provide us the probabilities of the data point belonging to each of the possible clusters.\n",
        "\n",
        "In Sklearn, $gmm.predict(X)$ the model assigns every data point to one of the clusters and $gmm.predict\\_proba(X)$ function return the probabilities that a data point belongs to each of the K clusters.\n",
        "\n",
        "Sklearn's GaussianMixture also comes with different options to constrain the covariance of the difference classes estimated: spherical, diagonal, tied or full covariance.\n",
        "\n",
        "### <u>Gaussian Mixture Models At A Glance</u>\n",
        "\n",
        "As the name implies, a Gaussian mixture model involves the mixture (i.e. superposition) of multiple Gaussian distributions. **Here rather than identifying clusters by “nearest” centroids, we fit a set of k gaussians to the data. And we estimate gaussian distribution parameters such as mean and Variance for each cluster and weight of a cluster. After learning the parameters for each data point we can calculate the probabilities of it belonging to each of the clusters.**\n",
        "\n",
        "![1_lTv7e4Cdlp738X_WFZyZHA.png](attachment:1_lTv7e4Cdlp738X_WFZyZHA.png)\n",
        "\n",
        "**Every distribution is multiplied by a weight $\\pi$($\\pi_1 + \\pi_2 + \\pi_3 = 1$) to account for the fact that we do not have an equal number of samples from each category.** In other words, we might only have included 1000 people from the red cluster class and 100,000 people from the green cluster class.\n",
        "\n",
        "### <u>Expectation Maximization</u>\n",
        "\n",
        "**<u>Expectation</u>**\n",
        "\n",
        "The first step, known as the expectation step or $E$ step, consists of **calculating the expectation of the component assignments $C_k$ for each data point $x_i \\in X$ given the model parameters $\\pi_k$ $\\mu_k$ and $\\sigma_k$.**\n",
        "\n",
        "**<u>Maximization</u>**\n",
        "\n",
        "The second step is known as the maximization step or $M$ step, which consists of **maximizing the expectations calculated in the E step with respect to the model parameters**. This step consists of updating the values $\\pi_k$, $\\mu_k$ and $\\sigma_k$.\n",
        "\n",
        "The entire iterative process repeats until the algorithm converges, giving a maximum likelihood estimate. Intuitively, **the algorithm works because knowing the component assignment $C_k$ for each $x_i$ makes solving for $\\pi_k$ $\\mu_k$ and $\\sigma_k$ easy, while knowing $\\pi_k$ $\\mu_k$  $\\sigma_k$ makes inferring $p(C_k|x_i)$ easy. The expectation step corresponds to the latter case while the maximization step corresponds to the former.** Thus, by alternating between which values are assumed fixed, or known, maximum likelihood estimates of the non-fixed values can be calculated in an efficient manner.\n",
        "\n",
        "### <u>Algorithm</u>\n",
        "- Initialize the mean $\\mu_k$, the covariance matrix $\\Sigma_k$ and the mixing coefficients $\\pi_k$ by some random values(or other values).\n",
        "- Compute the $C_k$ values for all k.\n",
        "- Again Estimate all the parameters using the current $C_k$ values.\n",
        "- Compute log-likelihood function.\n",
        "- Put some convergence criterion\n",
        "- If the log-likelihood value converges to some value (or if all the parameters converge to some values) then stop, else return to Step 2.\n",
        "\n",
        "This algorithm only guarantee that we land to a local optimal point, but it do not guarantee that this local optima is also the global one. And so, if the algorithm starts from different initialization points, in general it lands into different configurations.\n",
        "\n",
        "\n",
        "### <u>Example</u>"
      ],
      "metadata": {
        "id": "zI3kk_QpaSkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-11-15T05:16:08.573948Z",
          "iopub.execute_input": "2024-11-15T05:16:08.574352Z",
          "iopub.status.idle": "2024-11-15T05:16:08.611165Z",
          "shell.execute_reply.started": "2024-11-15T05:16:08.574288Z",
          "shell.execute_reply": "2024-11-15T05:16:08.6103Z"
        },
        "trusted": true,
        "id": "0RNwmM_XaSkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import DataFrame\n",
        "from sklearn.preprocessing import StandardScaler, normalize\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-15T05:16:32.957556Z",
          "iopub.execute_input": "2024-11-15T05:16:32.957888Z",
          "iopub.status.idle": "2024-11-15T05:16:34.868658Z",
          "shell.execute_reply.started": "2024-11-15T05:16:32.95784Z",
          "shell.execute_reply": "2024-11-15T05:16:34.867634Z"
        },
        "trusted": true,
        "id": "cdt42-wPaSkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df = pd.read_csv('../input/ccdata/CC GENERAL.csv')\n",
        "raw_df = raw_df.drop('CUST_ID', axis = 1)\n",
        "raw_df.fillna(method ='ffill', inplace = True)\n",
        "raw_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-15T05:16:53.890243Z",
          "iopub.execute_input": "2024-11-15T05:16:53.890593Z",
          "iopub.status.idle": "2024-11-15T05:16:53.955748Z",
          "shell.execute_reply.started": "2024-11-15T05:16:53.890541Z",
          "shell.execute_reply": "2024-11-15T05:16:53.954656Z"
        },
        "trusted": true,
        "id": "OmoPwS-DaSkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize data\n",
        "scaler = StandardScaler()\n",
        "scaled_df = scaler.fit_transform(raw_df)\n",
        "\n",
        "# Normalizing the Data\n",
        "normalized_df = normalize(scaled_df)\n",
        "\n",
        "# Converting the numpy array into a pandas DataFrame\n",
        "normalized_df = pd.DataFrame(normalized_df)\n",
        "\n",
        "# Reducing the dimensions of the data\n",
        "pca = PCA(n_components = 2)\n",
        "X_principal = pca.fit_transform(normalized_df)\n",
        "X_principal = pd.DataFrame(X_principal)\n",
        "X_principal.columns = ['P1', 'P2']\n",
        "\n",
        "X_principal.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-15T05:18:23.81846Z",
          "iopub.execute_input": "2024-11-15T05:18:23.818945Z",
          "iopub.status.idle": "2024-11-15T05:18:24.061661Z",
          "shell.execute_reply.started": "2024-11-15T05:18:23.81888Z",
          "shell.execute_reply": "2024-11-15T05:18:24.06028Z"
        },
        "trusted": true,
        "id": "Id5Sm_-1aSkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gmm = GaussianMixture(n_components = 3)\n",
        "gmm.fit(X_principal)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-15T05:18:44.855983Z",
          "iopub.execute_input": "2024-11-15T05:18:44.856329Z",
          "iopub.status.idle": "2024-11-15T05:18:44.963566Z",
          "shell.execute_reply.started": "2024-11-15T05:18:44.856276Z",
          "shell.execute_reply": "2024-11-15T05:18:44.9622Z"
        },
        "trusted": true,
        "id": "AreqRRkjaSkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the clustering\n",
        "plt.scatter(X_principal['P1'], X_principal['P2'],\n",
        "           c = GaussianMixture(n_components = 3).fit_predict(X_principal), cmap =plt.cm.winter, alpha = 0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-15T05:18:49.008991Z",
          "iopub.execute_input": "2024-11-15T05:18:49.009341Z",
          "iopub.status.idle": "2024-11-15T05:18:49.820587Z",
          "shell.execute_reply.started": "2024-11-15T05:18:49.00929Z",
          "shell.execute_reply": "2024-11-15T05:18:49.819522Z"
        },
        "trusted": true,
        "id": "CJ9G0hTNaSkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <u>Number of components(k) selection</u>:\n",
        "\n",
        "#### <u>Silhouette score</u>\n",
        "\n",
        "Silhouette score checks how much the clusters are compact and well separated. The more the score is near to one, the better the clustering is. Read more [here](https://www.kaggle.com/vipulgandhi/kmeans-detailed-explanation)\n",
        "\n",
        "Since we already know that the fitting procedure is not deterministic, we run twenty fits for each number of clusters, then we consider the mean value and the standard deviation of the best five runs."
      ],
      "metadata": {
        "id": "e-dc2YZaaSkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SelBest(arr:list, X:int)->list:\n",
        "    '''\n",
        "    returns the set of X configurations with shorter distance\n",
        "    '''\n",
        "    dx=np.argsort(arr)[:X]\n",
        "    return arr[dx]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-15T05:23:21.388122Z",
          "iopub.execute_input": "2024-11-15T05:23:21.388521Z",
          "iopub.status.idle": "2024-11-15T05:23:21.394845Z",
          "shell.execute_reply.started": "2024-11-15T05:23:21.388445Z",
          "shell.execute_reply": "2024-11-15T05:23:21.393593Z"
        },
        "trusted": true,
        "id": "_NsK3oHeaSkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters=np.arange(2, 8)\n",
        "sils=[]\n",
        "sils_err=[]\n",
        "iterations=20\n",
        "for n in n_clusters:\n",
        "    tmp_sil=[]              # storing silhouette score\n",
        "    for _ in range(iterations):\n",
        "        gmm=GaussianMixture(n, n_init=2).fit(X_principal)\n",
        "        labels=gmm.predict(X_principal)\n",
        "        sil=metrics.silhouette_score(X_principal, labels, metric='euclidean')\n",
        "        tmp_sil.append(sil)\n",
        "    val=np.mean(SelBest(np.array(tmp_sil), int(iterations/5)))\n",
        "    err=np.std(tmp_sil)\n",
        "    sils.append(val)\n",
        "    sils_err.append(err)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-15T05:23:24.201575Z",
          "iopub.execute_input": "2024-11-15T05:23:24.202015Z",
          "iopub.status.idle": "2024-11-15T05:27:06.59393Z",
          "shell.execute_reply.started": "2024-11-15T05:23:24.201928Z",
          "shell.execute_reply": "2024-11-15T05:27:06.59294Z"
        },
        "trusted": true,
        "id": "sqwywvrNaSkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.errorbar(n_clusters, sils, yerr=sils_err)\n",
        "plt.title(\"Silhouette Scores\", fontsize=20)\n",
        "plt.xticks(n_clusters)\n",
        "plt.xlabel(\"N. of clusters\")\n",
        "plt.ylabel(\"Score\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-15T05:34:42.286364Z",
          "iopub.execute_input": "2024-11-15T05:34:42.286812Z",
          "iopub.status.idle": "2024-11-15T05:34:42.629664Z",
          "shell.execute_reply.started": "2024-11-15T05:34:42.286754Z",
          "shell.execute_reply": "2024-11-15T05:34:42.627873Z"
        },
        "trusted": true,
        "id": "nZzTq9OVaSkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <u>Distance between GMMs</u>\n",
        "\n",
        "Here we form two datasets, each with an half randomly choose amount of data. We will then check how much the GMMs trained on the two sets are similar, for each configuration.\n",
        "\n",
        "Since we are talking about distributions, the concept of similarity is embedded in the Jensen-Shannon (JS) metric. **The lesser is the JS-distance between the two GMMs, the more the GMMs agree on how to fit the data.**\n",
        "\n",
        "**The lower the distance, the better the cluster.**"
      ],
      "metadata": {
        "id": "IVZb0CVOaSkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Courtesy of https://stackoverflow.com/questions/26079881/kl-divergence-of-two-gmms.\n",
        "# Here the difference is that we take the squared root, so it's a proper metric\n",
        "\n",
        "def gmm_js(gmm_p, gmm_q, n_samples=10**5):\n",
        "    X = gmm_p.sample(n_samples)[0]\n",
        "    log_p_X = gmm_p.score_samples(X)\n",
        "    log_q_X = gmm_q.score_samples(X)\n",
        "    log_mix_X = np.logaddexp(log_p_X, log_q_X)\n",
        "\n",
        "    Y = gmm_q.sample(n_samples)[0]\n",
        "    log_p_Y = gmm_p.score_samples(Y)\n",
        "    log_q_Y = gmm_q.score_samples(Y)\n",
        "    log_mix_Y = np.logaddexp(log_p_Y, log_q_Y)\n",
        "\n",
        "    return np.sqrt((log_p_X.mean() - (log_mix_X.mean() - np.log(2))\n",
        "            + log_q_Y.mean() - (log_mix_Y.mean() - np.log(2))) / 2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-15T05:38:17.319946Z",
          "iopub.execute_input": "2024-11-15T05:38:17.320318Z",
          "iopub.status.idle": "2024-11-15T05:38:17.331155Z",
          "shell.execute_reply.started": "2024-11-15T05:38:17.320258Z",
          "shell.execute_reply": "2024-11-15T05:38:17.329906Z"
        },
        "trusted": true,
        "id": "zNqhic38aSkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters=np.arange(2, 8)\n",
        "iterations=20\n",
        "results=[]\n",
        "res_sigs=[]\n",
        "for n in n_clusters:\n",
        "    dist=[]\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        train, test=train_test_split(X_principal, test_size=0.5)\n",
        "\n",
        "        gmm_train=GaussianMixture(n, n_init=2).fit(train)\n",
        "        gmm_test=GaussianMixture(n, n_init=2).fit(test)\n",
        "        dist.append(gmm_js(gmm_train, gmm_test))\n",
        "    selec=SelBest(np.array(dist), int(iterations/5))\n",
        "    result=np.mean(selec)\n",
        "    res_sig=np.std(selec)\n",
        "    results.append(result)\n",
        "    res_sigs.append(res_sig)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-15T05:38:20.928959Z",
          "iopub.execute_input": "2024-11-15T05:38:20.929387Z",
          "iopub.status.idle": "2024-11-15T05:39:52.820223Z",
          "shell.execute_reply.started": "2024-11-15T05:38:20.929317Z",
          "shell.execute_reply": "2024-11-15T05:39:52.818799Z"
        },
        "trusted": true,
        "id": "jryxnAXpaSkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.errorbar(n_clusters, results, yerr=res_sigs)\n",
        "plt.title(\"SQRT Distance between Train and Test GMMs\", fontsize=20)\n",
        "plt.xticks(n_clusters)\n",
        "plt.xlabel(\"N. of clusters\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-15T05:44:13.707231Z",
          "iopub.execute_input": "2024-11-15T05:44:13.707589Z",
          "iopub.status.idle": "2024-11-15T05:44:13.992076Z",
          "shell.execute_reply.started": "2024-11-15T05:44:13.70753Z",
          "shell.execute_reply": "2024-11-15T05:44:13.990372Z"
        },
        "trusted": true,
        "id": "nNbCvrOLaSkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bayesian information criterion (BIC)\n",
        "\n",
        "**This criterion gives us an estimation on how much is good the GMM in terms of predicting the data we actually have. The lower is the BIC, the better is the model to actually predict the data we have**.\n",
        "*In order to avoid overfitting, this technique penalizes models with big number of clusters.*"
      ],
      "metadata": {
        "id": "E-agS9RBaSkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters=np.arange(2, 8)\n",
        "bics=[]\n",
        "bics_err=[]\n",
        "iterations=20\n",
        "for n in n_clusters:\n",
        "    tmp_bic=[]\n",
        "    for _ in range(iterations):\n",
        "        gmm=GaussianMixture(n, n_init=2).fit(X_principal)\n",
        "\n",
        "        tmp_bic.append(gmm.bic(X_principal))\n",
        "    val=np.mean(SelBest(np.array(tmp_bic), int(iterations/5)))\n",
        "    err=np.std(tmp_bic)\n",
        "    bics.append(val)\n",
        "    bics_err.append(err)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-15T05:46:16.360416Z",
          "iopub.execute_input": "2024-11-15T05:46:16.360855Z",
          "iopub.status.idle": "2024-11-15T05:47:03.450145Z",
          "shell.execute_reply.started": "2024-11-15T05:46:16.360777Z",
          "shell.execute_reply": "2024-11-15T05:47:03.449119Z"
        },
        "trusted": true,
        "id": "7p--MMkOaSkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.errorbar(n_clusters,bics, yerr=bics_err, label='BIC')\n",
        "plt.title(\"BIC Scores\", fontsize=20)\n",
        "plt.xticks(n_clusters)\n",
        "plt.xlabel(\"N. of clusters\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-15T05:49:37.455948Z",
          "iopub.execute_input": "2024-11-15T05:49:37.456344Z",
          "iopub.status.idle": "2024-11-15T05:49:37.674933Z",
          "shell.execute_reply.started": "2024-11-15T05:49:37.456291Z",
          "shell.execute_reply": "2024-11-15T05:49:37.673878Z"
        },
        "trusted": true,
        "id": "44-sV0u9aSkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following this criterion, the bigger the number of clusters, the better should be the model. Which means that the penalty BIC criteria gives to complex models do not save us from overfit.\n",
        "\n",
        "But before screaming and trashing out this technique, **we can notice two things. The first is that the curve is fairly smooth and monotone. The second is that the curve follows different slopes in different part of it.** Starting from these two observations, the temptation to check where the BIC curve change slope is big. So let’s check it!\n",
        "\n",
        "Technically, we have to calculate the gradient of the BIC scores curve. Intuitively, the concept of gradient is simple: if two consecutive points have the same value, their gradient is zero. If they have different values, their gradient can be eighter negative, if the second point has a lower value, or positive otherwise. The magnitude of the gradient tells us how much the two values are different.\n"
      ],
      "metadata": {
        "id": "902CaOJDaSkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.errorbar(n_clusters, np.gradient(bics), yerr=bics_err, label='BIC')\n",
        "plt.title(\"Gradient of BIC Scores\", fontsize=20)\n",
        "plt.xticks(n_clusters)\n",
        "plt.xlabel(\"N. of clusters\")\n",
        "plt.ylabel(\"grad(BIC)\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-15T05:51:01.54652Z",
          "iopub.execute_input": "2024-11-15T05:51:01.546903Z",
          "iopub.status.idle": "2024-11-15T05:51:01.935409Z",
          "shell.execute_reply.started": "2024-11-15T05:51:01.546831Z",
          "shell.execute_reply": "2024-11-15T05:51:01.933896Z"
        },
        "trusted": true,
        "id": "hv_CmGW7aSkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bSFtuXeEaSkZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}